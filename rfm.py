import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# === 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ ===
transactions = pd.read_csv("/Users/dana/Downloads/test/st/transactions.csv")
transactions.columns = [c.strip().lower().replace(" ", "_") for c in transactions.columns]
transactions["date"] = pd.to_datetime(transactions["date"], errors="coerce")
transactions["category"] = transactions["category"].astype(str).str.lower().str.strip()

now = pd.Timestamp(datetime(2025, 10, 18))

# === 2. RFM ===
rfm = (
    transactions.groupby("user_id")
    .agg(
        last_tx=("date", "max"),
        first_tx=("date", "min"),
        F=("transaction_id", "count"),
        M=("amount", "sum")
    )
    .reset_index()
)
rfm["R"] = (now - rfm["last_tx"]).dt.days.fillna(9999)
rfm["activity_span"] = (rfm["last_tx"] - rfm["first_tx"]).dt.days.fillna(0)
rfm.drop(columns=["first_tx", "last_tx"], inplace=True)

# === 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—É–º–º–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ===
user_totals = transactions.groupby("user_id")["amount"].sum()
transactions = transactions.merge(user_totals, on="user_id", suffixes=("", "_user_total"))
transactions["amount_share"] = transactions["amount"] / transactions["amount_user_total"]

# === 4. –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ –ø–æ —Å—É–º–º–µ —Ä–∞—Å—Ö–æ–¥–æ–≤ ===
def category_share(cat_list):
    mask = transactions["category"].isin(cat_list)
    return transactions[mask].groupby("user_id")["amount_share"].sum().fillna(0)

features = rfm.copy()
features["Z_charity"] = category_share(["charity", "donation", "zakat", "waqf"])
features["Edu"] = category_share(["education"])
features["Biz"] = category_share(["business"])
features["Prop"] = category_share(["property", "–∏–º—É—â–µ—Å—Ç–≤–æ"])
features["Health"] = category_share(["healthcare"])
features["Deposit"] = category_share(["deposit", "card"])
features.fillna(0, inplace=True)

# === 5. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å ===
r_thresh = features["R"].quantile(0.8)
f_thresh = features["F"].quantile(0.2)

def classify_activity(row):
    if row["R"] > r_thresh and row["F"] < f_thresh:
        return "–ó–∞—Å—ã–ø–∞—é—â–∏–π"
    elif row["R"] < features["R"].quantile(0.3):
        return "–ü—Ä–æ—Å—ã–ø–∞—é—â–∏–π—Å—è"
    else:
        return "–ê–∫—Ç–∏–≤–Ω—ã–π"

features["activity_stage"] = features.apply(classify_activity, axis=1)

# === 6. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–π (goals) ===
def detect_goal(user_tx):
    cats = user_tx["category"].value_counts(normalize=True)
    if cats.get("property", 0) + cats.get("–∏–º—É—â–µ—Å—Ç–≤–æ", 0) > 0.35:
        return "–ü–æ–∫—É–ø–∫–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"
    elif cats.get("auto", 0) > 0.3:
        return "–ü–æ–∫—É–ø–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è"
    elif cats.get("education", 0) > 0.3:
        return "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"
    elif cats.get("business", 0) > 0.3:
        return "–†–∞–∑–≤–∏—Ç–∏–µ –±–∏–∑–Ω–µ—Å–∞"
    elif cats.get("charity", 0) + cats.get("zakat", 0) > 0.2:
        return "–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
    elif cats.get("deposit", 0) + cats.get("card", 0) > 0.4:
        return "–ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ"
    elif cats.get("healthcare", 0) > 0.25:
        return "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∑–∞—â–∏—Ç–∞"
    else:
        return "–û–±—â–∏–µ —Ä–∞—Å—Ö–æ–¥—ã"

goal_map = transactions.groupby("user_id", group_keys=False).apply(detect_goal).reset_index()
goal_map.columns = ["user_id", "goal"]
features = features.merge(goal_map, on="user_id", how="left")

# === 7. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ PCA ===
scaler = StandardScaler()
scaled = scaler.fit_transform(features[["R", "F", "M", "Z_charity", "Edu", "Biz", "Prop", "Health", "Deposit"]])

pca = PCA(n_components=0.95, random_state=42)
reduced = pca.fit_transform(scaled)

# === 8. Gaussian Mixture ===
gmm = GaussianMixture(n_components=8, covariance_type="full", random_state=42)
features["cluster"] = gmm.fit_predict(reduced)

# === 9. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∞–≤—Ç–æ-–ª–µ–π–±–ª–æ–≤ ===
def above_dynamic_threshold(row, col):
    return row[col] > features[col].quantile(0.75)

def label_cluster(row):
    if above_dynamic_threshold(row, "Z_charity"):
        return "–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ª–∏–¥–µ—Ä"
    elif above_dynamic_threshold(row, "Edu"):
        return "–†–∞–∑–≤–∏–≤–∞—é—â–∏–π—Å—è —É—á–µ–Ω–∏–∫"
    elif above_dynamic_threshold(row, "Biz"):
        return "–ë–∏–∑–Ω–µ—Å-–∏–Ω–≤–µ—Å—Ç–æ—Ä"
    elif above_dynamic_threshold(row, "Prop"):
        return "–°–µ–º–µ–π–Ω—ã–π —Å—Ç—Ä–æ–∏—Ç–µ–ª—å"
    elif above_dynamic_threshold(row, "Health"):
        return "–ó–∞–±–æ—Ç—è—â–∏–π—Å—è –æ –∑–¥–æ—Ä–æ–≤—å–µ"
    elif above_dynamic_threshold(row, "Deposit"):
        return "–§–∏–Ω–∞–Ω—Å–æ–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π"
    elif row["F"] > features["F"].quantile(0.8):
        return "–°–≤–µ—Ä—Ö–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"
    elif row["R"] > features["R"].quantile(0.8):
        return "–ó–∞—Å—ã–ø–∞—é—â–∏–π"
    else:
        return "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç"

features["segment"] = features.apply(label_cluster, axis=1)

# === 10. –°–æ–≤–º–µ—â–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞ ===
features.loc[features["activity_stage"] == "–ó–∞—Å—ã–ø–∞—é—â–∏–π", "segment"] = "–ó–∞—Å—ã–ø–∞—é—â–∏–π"
features.loc[features["activity_stage"] == "–ü—Ä–æ—Å—ã–ø–∞—é—â–∏–π—Å—è", "segment"] = "–ü—Ä–æ—Å—ã–ø–∞—é—â–∏–π—Å—è"

# === 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ===
features.to_csv("smart_segments_final.csv", index=False, encoding="utf-8-sig")

print("‚úÖ –£–º–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
print(features[["user_id", "segment", "goal", "activity_stage"]].head(20))

# === 12. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ ===
plt.figure(figsize=(10,5))
sns.countplot(x="segment", data=features, order=features["segment"].value_counts().index)
plt.xticks(rotation=45)
plt.title("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
plt.tight_layout()
plt.show()
# ...existing code...

# === 11.5. –°–æ–±–∏—Ä–∞–µ–º –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (–∫–∞–∂–¥–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è + —Ñ–∏—á–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è) ===
enrich_cols = [
    "user_id", "segment", "goal", "activity_stage", "cluster",
    "R", "F", "M", "Z_charity", "Edu", "Biz", "Prop", "Health", "Deposit"
]
# —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ features
enrich_cols = [c for c in enrich_cols if c in features.columns]

enriched_transactions = transactions.merge(
    features[enrich_cols],
    on="user_id",
    how="left",
    validate="m:1"  # many transactions -> one user row
)

# —Å–æ—Ö—Ä–∞–Ω—è–µ–º
enriched_transactions.to_csv("enriched_transactions.csv", index=False, encoding="utf-8-sig")
# (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Å–æ—Ö—Ä–∞–Ω—è–µ–º user-level –¥–∞—Ç–∞—Å–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
features.to_csv("user_features.csv", index=False, encoding="utf-8-sig")

print("‚úÖ Enriched transactions saved to enriched_transactions.csv")
# ...existing code...
